{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1a2f36-1827-43c3-9db6-a7d8301cea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.colors as mcolors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287bdc4-2013-4767-89e6-0f02aa148c98",
   "metadata": {},
   "source": [
    "# Q1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f471b19-1329-4a4c-b2af-a46b93587523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Action Space: Discrete(3)\n",
      "Step: 10000\n",
      "State: [-0.35644105  0.00581925]\n",
      "Action: 0\n",
      "Reward: -1.0\n",
      "------------------------------\n",
      "Step: 20000\n",
      "State: [-1.036639    0.02862402]\n",
      "Action: 2\n",
      "Reward: -1.0\n",
      "------------------------------\n",
      "Step: 30000\n",
      "State: [-0.5471804   0.03507504]\n",
      "Action: 1\n",
      "Reward: -1.0\n",
      "------------------------------\n",
      "Total reward: -34695.0\n"
     ]
    }
   ],
   "source": [
    "# Load the MountainCar-v0 environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# Print the state and action space\n",
    "print(\"State Space:\", env.observation_space)\n",
    "print(\"Action Space:\", env.action_space)\n",
    "\n",
    "# Initialize the environment\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "# Random agent loop to understand reward function\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    next_state, reward, done, info,_ = env.step(action)\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    if steps%10000==0:\n",
    "        print(f\"Step: {steps}\")\n",
    "        print(f\"State: {next_state}\")\n",
    "        print(f\"Action: {action}\")\n",
    "        print(f\"Reward: {reward}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "print(\"Total reward:\", total_reward)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39bdf2-12b0-4061-bc93-7dcaa16e437a",
   "metadata": {},
   "source": [
    "# Observations\n",
    "**State Space**: The state space consists of two continuous variables:\n",
    "\n",
    "Position of the car (ranging from -1.2 to 0.6)\n",
    "Velocity of the car (ranging from -0.07 to 0.07)\n",
    "Action Space: The action space is discrete with three possible actions:\n",
    "\n",
    "0: Push the car left\n",
    "1: Do nothing\n",
    "2: Push the car right\n",
    "\n",
    "**Reward**: For each time step, the agent receives a reward of -1 until it reaches the goal at the top of the mountain on the right (position >= 0.5), at which point the episode terminates. The constant negative reward encourages the agent to reach the goal as quickly as possible to minimize the number of time steps.\n",
    "\n",
    "**Objective**: The random agent does not learn or optimize behavior, so it typically takes a high number of steps without reaching the goal due to random action selection, highlighting the need for a more intelligent policy for success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40ca3a-b0a5-433d-aa27-84c5d16570a0",
   "metadata": {},
   "source": [
    "# Q1(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1852ca91-34e0-4876-b15c-35884e8eeeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # ouptut layer w\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = self.out(x)         # Calculate output\n",
    "        return x\n",
    "\n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c4f9cc-12ca-4c85-93ad-8bc37e44e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (adjustable)\n",
    "learning_rate_a = 0.01         # learning rate (alpha)\n",
    "discount_factor_g = 0.9         # discount rate (gamma)    \n",
    "network_sync_rate = 50000          # number of steps the agent takes before syncing the policy and target network\n",
    "replay_memory_size = 100000       # size of replay memory\n",
    "mini_batch_size = 32            # size of the training data set sampled from the replay memory\n",
    "\n",
    "num_divisions = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0105f46c-90e8-4997-a43c-65588614a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1-1000 Epsilon:1 Best Reward:-1000.0 Mean Reward:-1000.0 Best Mean Reward:-1000.0\n",
      "Epoch:1001-2000 Epsilon:1 Best Reward:-1000.0 Mean Reward:-1000.0 Best Mean Reward:-1000.0\n",
      "Epoch:2001-3000 Epsilon:1 Best Reward:-1000.0 Mean Reward:-1000.0 Best Mean Reward:-1000.0\n",
      "Epoch:3001-4000 Epsilon:1 Best Reward:-1000.0 Mean Reward:-1000.0 Best Mean Reward:-1000.0\n",
      "Epoch:4001-5000 Epsilon:0.99 Best Reward:-685.0 Mean Reward:-999.93 Best Mean Reward:-999.93\n",
      "Epoch:5001-6000 Epsilon:0.97 Best Reward:-685.0 Mean Reward:-999.93 Best Mean Reward:-999.93\n",
      "Epoch:6001-7000 Epsilon:0.95 Best Reward:-685.0 Mean Reward:-999.84 Best Mean Reward:-999.84\n",
      "Epoch:7001-8000 Epsilon:0.93 Best Reward:-685.0 Mean Reward:-999.83 Best Mean Reward:-999.83\n",
      "Epoch:8001-9000 Epsilon:0.91 Best Reward:-685.0 Mean Reward:-999.57 Best Mean Reward:-999.57\n",
      "Epoch:9001-10000 Epsilon:0.89 Best Reward:-594.0 Mean Reward:-999.55 Best Mean Reward:-999.55\n",
      "Epoch:10001-11000 Epsilon:0.87 Best Reward:-594.0 Mean Reward:-999.16 Best Mean Reward:-999.16\n",
      "Epoch:11001-12000 Epsilon:0.85 Best Reward:-396.0 Mean Reward:-998.71 Best Mean Reward:-998.71\n",
      "Epoch:12001-13000 Epsilon:0.83 Best Reward:-396.0 Mean Reward:-997.62 Best Mean Reward:-997.62\n",
      "Epoch:13001-14000 Epsilon:0.81 Best Reward:-365.0 Mean Reward:-997.12 Best Mean Reward:-997.12\n",
      "Epoch:14001-15000 Epsilon:0.79 Best Reward:-365.0 Mean Reward:-997.21 Best Mean Reward:-997.12\n",
      "Epoch:15001-16000 Epsilon:0.77 Best Reward:-327.0 Mean Reward:-991.53 Best Mean Reward:-991.53\n",
      "Epoch:16001-17000 Epsilon:0.75 Best Reward:-327.0 Mean Reward:-990.37 Best Mean Reward:-990.37\n",
      "Epoch:17001-18000 Epsilon:0.73 Best Reward:-327.0 Mean Reward:-989.7 Best Mean Reward:-989.7\n",
      "Epoch:18001-19000 Epsilon:0.71 Best Reward:-254.0 Mean Reward:-977.3 Best Mean Reward:-977.3\n",
      "Epoch:19001-20000 Epsilon:0.69 Best Reward:-245.0 Mean Reward:-972.9 Best Mean Reward:-972.9\n",
      "Epoch:20001-21000 Epsilon:0.67 Best Reward:-245.0 Mean Reward:-976.57 Best Mean Reward:-972.9\n",
      "Epoch:21001-22000 Epsilon:0.65 Best Reward:-245.0 Mean Reward:-977.85 Best Mean Reward:-972.9\n",
      "Epoch:22001-23000 Epsilon:0.63 Best Reward:-245.0 Mean Reward:-977.06 Best Mean Reward:-972.9\n",
      "Epoch:23001-24000 Epsilon:0.61 Best Reward:-245.0 Mean Reward:-988.93 Best Mean Reward:-972.9\n",
      "Epoch:24001-25000 Epsilon:0.59 Best Reward:-245.0 Mean Reward:-993.31 Best Mean Reward:-972.9\n",
      "Epoch:25001-26000 Epsilon:0.57 Best Reward:-245.0 Mean Reward:-992.71 Best Mean Reward:-972.9\n",
      "Epoch:26001-27000 Epsilon:0.55 Best Reward:-245.0 Mean Reward:-993.0 Best Mean Reward:-972.9\n",
      "Epoch:27001-28000 Epsilon:0.53 Best Reward:-245.0 Mean Reward:-995.49 Best Mean Reward:-972.9\n",
      "Epoch:28001-29000 Epsilon:0.51 Best Reward:-229.0 Mean Reward:-982.43 Best Mean Reward:-972.9\n",
      "Epoch:29001-30000 Epsilon:0.49 Best Reward:-229.0 Mean Reward:-982.38 Best Mean Reward:-972.9\n",
      "Epoch:30001-31000 Epsilon:0.47 Best Reward:-229.0 Mean Reward:-985.08 Best Mean Reward:-972.9\n",
      "Epoch:31001-32000 Epsilon:0.45 Best Reward:-206.0 Mean Reward:-983.4 Best Mean Reward:-972.9\n",
      "Epoch:32001-33000 Epsilon:0.43 Best Reward:-206.0 Mean Reward:-982.07 Best Mean Reward:-972.9\n",
      "Epoch:33001-34000 Epsilon:0.41 Best Reward:-206.0 Mean Reward:-996.41 Best Mean Reward:-972.9\n",
      "Epoch:34001-35000 Epsilon:0.39 Best Reward:-206.0 Mean Reward:-996.35 Best Mean Reward:-972.9\n",
      "Epoch:35001-36000 Epsilon:0.37 Best Reward:-206.0 Mean Reward:-990.55 Best Mean Reward:-972.9\n",
      "Epoch:36001-37000 Epsilon:0.35 Best Reward:-206.0 Mean Reward:-980.98 Best Mean Reward:-972.9\n",
      "Epoch:37001-38000 Epsilon:0.33 Best Reward:-206.0 Mean Reward:-982.15 Best Mean Reward:-972.9\n",
      "Epoch:38001-39000 Epsilon:0.31 Best Reward:-206.0 Mean Reward:-972.7 Best Mean Reward:-972.7\n",
      "Epoch:39001-40000 Epsilon:0.29 Best Reward:-206.0 Mean Reward:-958.62 Best Mean Reward:-958.62\n",
      "Best rewards so far: -173.0\n",
      "Best model saved as mountaincar_dql_40680.pt\n",
      "Best rewards so far: -169.0\n",
      "Best model saved as mountaincar_dql_40689.pt\n",
      "Epoch:40001-41000 Epsilon:0.27 Best Reward:-169.0 Mean Reward:-928.31 Best Mean Reward:-928.31\n",
      "Best rewards so far: -168.0\n",
      "Best model saved as mountaincar_dql_41035.pt\n",
      "Best rewards so far: -166.0\n",
      "Best model saved as mountaincar_dql_41081.pt\n",
      "Best rewards so far: -165.0\n",
      "Best model saved as mountaincar_dql_41120.pt\n",
      "Epoch:41001-42000 Epsilon:0.25 Best Reward:-165.0 Mean Reward:-910.93 Best Mean Reward:-910.93\n",
      "Best rewards so far: -163.0\n",
      "Best model saved as mountaincar_dql_42215.pt\n",
      "Best rewards so far: -160.0\n",
      "Best model saved as mountaincar_dql_42220.pt\n",
      "Best rewards so far: -155.0\n",
      "Best model saved as mountaincar_dql_42227.pt\n",
      "Best rewards so far: -154.0\n",
      "Best model saved as mountaincar_dql_42241.pt\n",
      "Epoch:42001-43000 Epsilon:0.23 Best Reward:-154.0 Mean Reward:-879.94 Best Mean Reward:-879.94\n",
      "Epoch:43001-44000 Epsilon:0.21 Best Reward:-154.0 Mean Reward:-865.12 Best Mean Reward:-865.12\n",
      "Best rewards so far: -146.0\n",
      "Best model saved as mountaincar_dql_44805.pt\n",
      "Epoch:44001-45000 Epsilon:0.19 Best Reward:-146.0 Mean Reward:-838.5 Best Mean Reward:-838.5\n",
      "Best rewards so far: -120.0\n",
      "Best model saved as mountaincar_dql_45122.pt\n",
      "Best rewards so far: -111.0\n",
      "Best model saved as mountaincar_dql_45258.pt\n",
      "Epoch:45001-46000 Epsilon:0.17 Best Reward:-111.0 Mean Reward:-808.27 Best Mean Reward:-808.27\n",
      "Epoch:46001-47000 Epsilon:0.15 Best Reward:-111.0 Mean Reward:-834.74 Best Mean Reward:-808.27\n",
      "Epoch:47001-48000 Epsilon:0.13 Best Reward:-111.0 Mean Reward:-865.96 Best Mean Reward:-808.27\n",
      "Epoch:48001-49000 Epsilon:0.11 Best Reward:-111.0 Mean Reward:-890.24 Best Mean Reward:-808.27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MountainCar Deep Q-Learning\n",
    "class MountainCarDQL():\n",
    "    # Neural Network\n",
    "    loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
    "    optimizer = None                # NN Optimizer. Initialize later.\n",
    "\n",
    "    def __init__(self,learning_rate=0.01,discount_factor_g = 0.9,network_sync_rate = 50000,replay_memory_size=100000,mini_batch_size = 32,num_divisions=20):\n",
    "         # Initialize placeholders for tracking\n",
    "        self.total_steps = 0\n",
    "        self.best_mean_reward = float('-inf')\n",
    "        self.mean_rewards_per_step = []\n",
    "        self.best_mean_rewards_per_step = []\n",
    "        self.mean_window = 1000\n",
    "        self.render = False\n",
    "\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor_g = discount_factor_g          \n",
    "        self.network_sync_rate = network_sync_rate         \n",
    "        self.replay_memory_size = replay_memory_size     \n",
    "        self.mini_batch_size = mini_batch_size           \n",
    "        self.num_divisions = num_divisions\n",
    "        \n",
    "        env = gym.make('MountainCar-v0', render_mode='human' if self.render else None)\n",
    "        \n",
    "        # Divide position and velocity into segments\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        self.epsilon = 1 # 1 = 100% random actions\n",
    "         # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
    "        self.rewards_per_episode = []\n",
    "\n",
    "        # List to keep track of epsilon decay\n",
    "        self.epsilon_history = []        \n",
    "\n",
    "    # Train the environment\n",
    "    def train(self, episodes, render=False):\n",
    "        # Create FrozenLake instance\n",
    "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
    "        num_actions = env.action_space.n\n",
    "        \n",
    "\n",
    "        memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "        target_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "\n",
    "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
    "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "        \n",
    "        # Policy network optimizer. \"Adam\" optimizer can be swapped to something else. \n",
    "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate)\n",
    "\n",
    "       \n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        step_count=0\n",
    "        goal_reached=False\n",
    "        best_rewards=-200\n",
    "            \n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "\n",
    "            rewards = 0\n",
    "\n",
    "            # Agent navigates map until it falls into hole/reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and rewards>-1000):\n",
    "\n",
    "                # Select action based on epsilon-greedy\n",
    "                if random.random() < self.epsilon:\n",
    "                    # select random action\n",
    "                    action = env.action_space.sample() # actions: 0=left,1=idle,2=right\n",
    "                else:\n",
    "                    # select best action            \n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "                # Accumulate reward\n",
    "                rewards += reward\n",
    "\n",
    "                # Save experience into memory\n",
    "                memory.append((state, action, new_state, reward, terminated)) \n",
    "\n",
    "                # Move to the next state\n",
    "                state = new_state\n",
    "\n",
    "                # Increment step counter\n",
    "                step_count+=1\n",
    "                self.total_steps += 1\n",
    "\n",
    "            # Keep track of the rewards collected per episode.\n",
    "            self.rewards_per_episode.append(rewards)\n",
    "            if(terminated):\n",
    "                goal_reached = True\n",
    "\n",
    "            # Graph training progress\n",
    "            if(i!=0 and i%1000==0):\n",
    "                mean_reward = np.mean(self.rewards_per_episode[-5000:])\n",
    "                self.mean_rewards_per_step.append((self.total_steps, np.round(mean_reward,2)))\n",
    "                \n",
    "                self.best_mean_reward = max(self.best_mean_reward, np.round(mean_reward,2))\n",
    "                self.best_mean_rewards_per_step.append((self.total_steps, self.best_mean_reward))\n",
    "\n",
    "                print(f'Epoch:{max(1,i-1000+1)}-{i} Epsilon:{np.round(self.epsilon,2)} Best Reward:{max(self.rewards_per_episode)} Mean Reward:{np.round(mean_reward,2)} Best Mean Reward:{np.round(self.best_mean_reward,2)}')     \n",
    "                self.plot_progress()\n",
    "            \n",
    "            if rewards>best_rewards:\n",
    "                best_rewards = rewards\n",
    "                print(f'Best rewards so far: {best_rewards}')\n",
    "                # Save policy\n",
    "                torch.save(policy_dqn.state_dict(), f\"mountaincar_dql_{i}.pt\")\n",
    "                print(f\"Best model saved as mountaincar_dql_{i}.pt\")\n",
    "\n",
    "            # Check if senough experience has been collected\n",
    "            if len(memory)>self.mini_batch_size and goal_reached:\n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)        \n",
    "\n",
    "                # Decay epsilon\n",
    "                self.epsilon = max(self.epsilon - 1/episodes, 0)\n",
    "                self.epsilon_history.append(self.epsilon)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                    step_count=0                    \n",
    "\n",
    "        # Close environment\n",
    "        env.close()\n",
    "        \n",
    "    def plot_progress(self):\n",
    "        # Extract the total_steps and rewards from tracked mean and best mean rewards\n",
    "        steps, mean_rewards = zip(*self.mean_rewards_per_step)\n",
    "        steps_best, best_mean_rewards = zip(*self.best_mean_rewards_per_step)\n",
    "        # import pdb;pdb.set_trace()\n",
    "    \n",
    "        plt.figure(figsize=(10, 6))\n",
    "    \n",
    "        # Plot mean rewards over time with thicker lines and add markers\n",
    "        plt.plot(steps, mean_rewards, color='blue', linestyle='-', linewidth=1.5, label='Mean Reward over 1000 Episodes', marker='o')\n",
    "    \n",
    "        # Plot best mean rewards over time with thicker lines and different color\n",
    "        plt.plot(steps_best, best_mean_rewards, color='orange', linestyle='-', linewidth=2, label='Best Mean Reward', marker='o')\n",
    "    \n",
    "        # # Set axis limits for a similar view\n",
    "        # plt.ylim(100, max(best_mean_rewards) + 20)\n",
    "        \n",
    "        # Add grid for better readability\n",
    "        plt.grid(True)\n",
    "    \n",
    "        # Add labels, title, and legend\n",
    "        plt.xlabel(\"Timesteps (Scaled)\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"DQN Training Performance\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "    \n",
    "        # Save and show the plot\n",
    "        plt.savefig(f'mean_and_best_mean_rewards_lr_{self.learning_rate}.png')\n",
    "        # plt.show()\n",
    "        plt.close()  # Close the figure\n",
    "\n",
    "   \n",
    "\n",
    "    def plot_action_choices(self, model_filepath):\n",
    "        # Load the trained policy network\n",
    "        policy_dqn = DQN(in_states=2, h1_nodes=10, out_actions=3)  # Adjust according to your DQN setup\n",
    "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
    "        policy_dqn.eval()  # Switch to evaluation mode\n",
    "\n",
    "        # Define position and velocity ranges based on environment's observation space\n",
    "        position_range = np.linspace(-1.2, 0.6, 100)  # Positions between -1.2 and 0.6\n",
    "        velocity_range = np.linspace(-0.07, 0.07, 100)  # Velocities between -0.07 and 0.07\n",
    "\n",
    "        # Create a mesh grid for position and velocity\n",
    "        pos_grid, vel_grid = np.meshgrid(position_range, velocity_range)\n",
    "        action_grid = np.zeros(pos_grid.shape, dtype=int)\n",
    "\n",
    "        # For each (position, velocity) pair, get the action chosen by the policy\n",
    "        for i in range(pos_grid.shape[0]):\n",
    "            for j in range(pos_grid.shape[1]):\n",
    "                # Convert position and velocity to state tensor\n",
    "                pos, vel = pos_grid[i, j], vel_grid[i, j]\n",
    "                state_tensor = self.state_to_dqn_input((pos, vel)).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "                # Predict action\n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(state_tensor).argmax().item()\n",
    "                action_grid[i, j] = action\n",
    "\n",
    "        # Define a custom colormap\n",
    "        cmap = mcolors.ListedColormap(['blue', 'green', 'red'])\n",
    "        bounds = [-0.5, 0.5, 1.5, 2.5]  # Boundaries for the actions\n",
    "        norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        # Plot the action choices on a 2D grid with custom colormap\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        contour = plt.contourf(pos_grid, vel_grid, action_grid, levels=bounds, cmap=cmap, norm=norm, alpha=0.6)\n",
    "        \n",
    "        # Create a color legend based on actions\n",
    "        action_labels = ['Accelerate Left (0)', 'Do Nothing (1)', 'Accelerate Right (2)']\n",
    "        \n",
    "        # Add custom legend\n",
    "        for i, label in enumerate(action_labels):\n",
    "            plt.scatter([], [], color=cmap(i), label=label)  # Dummy scatter for legend\n",
    "\n",
    "        plt.colorbar(contour, ticks=[0, 1, 2], label='Action')\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Velocity')\n",
    "        plt.title('Action Choices of the Trained Agent for Various Position and Velocity Values')\n",
    "        plt.legend(title=\"Actions\")\n",
    "        plt.show()\n",
    "        \n",
    "    # Optimize policy network\n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "\n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "            if terminated: \n",
    "                # Agent receive reward of 0 for reaching goal.\n",
    "                # When in a terminated state, target q value should be set to the reward.\n",
    "                target = torch.FloatTensor([reward])\n",
    "            else:\n",
    "                # Calculate target q value \n",
    "                with torch.no_grad():\n",
    "                    target = torch.FloatTensor(\n",
    "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state)).max()\n",
    "                    )\n",
    "\n",
    "            # Get the current set of Q values\n",
    "            current_q = policy_dqn(self.state_to_dqn_input(state))\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(self.state_to_dqn_input(state)) \n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "                \n",
    "        # Compute loss for the whole minibatch\n",
    "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    '''\n",
    "    Converts a state (position, velocity) to tensor representation.\n",
    "    Example:\n",
    "    Input = (0.3, -0.03)\n",
    "    Return = tensor([16, 6])\n",
    "    '''\n",
    "    def state_to_dqn_input(self, state)->torch.Tensor:\n",
    "        state_p = np.digitize(state[0], self.pos_space)\n",
    "        state_v = np.digitize(state[1], self.vel_space)\n",
    "        \n",
    "        return torch.FloatTensor([state_p, state_v])\n",
    "    \n",
    "    # Run the environment with the learned policy\n",
    "    def test(self, episodes, model_filepath):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('MountainCar-v0', render_mode='human')\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions) \n",
    "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
    "        policy_dqn.eval()    # switch model to evaluation mode\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions            \n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and not truncated):  \n",
    "                # Select best action   \n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mountaincar = MountainCarDQL()\n",
    "    mountaincar.train(50000, False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87608f65-3251-44d6-a6e5-f5c1afbf022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fs/1_sgn68n2914x1m2zltpc2m80000gn/T/ipykernel_44994/2266303609.py:173: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_dqn.load_state_dict(torch.load(model_filepath))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mountaincar.plot_action_choices('mountaincar_dql_45243.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb16fda-79fc-41c4-8786-39eb0b98072e",
   "metadata": {},
   "source": [
    "# Observations \n",
    "With the most optimized model trained we get the above graph showing us \n",
    "1. For Positions>0 The most probable action taken is accelerate right\n",
    "2. For Position<0 primarily with higher negative values the car is expected to accelerate left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfac88b8-ffd5-4ee2-b804-0d17a055d128",
   "metadata": {},
   "source": [
    "# Q1(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd2ad79-f4ba-45b8-9d85-d32945fd8fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Results for Learning Rate 0.1\n",
      "Epoch:1-1000 Epsilon:1.0 Best Reward:-473.0 Mean Reward:-999.44 Best Mean Reward:-999.44\n",
      "Epoch:1001-2000 Epsilon:0.98 Best Reward:-473.0 Mean Reward:-999.7 Best Mean Reward:-999.44\n",
      "Epoch:2001-3000 Epsilon:0.96 Best Reward:-473.0 Mean Reward:-999.8 Best Mean Reward:-999.44\n",
      "Epoch:3001-4000 Epsilon:0.94 Best Reward:-473.0 Mean Reward:-999.85 Best Mean Reward:-999.44\n",
      "Epoch:4001-5000 Epsilon:0.92 Best Reward:-473.0 Mean Reward:-999.86 Best Mean Reward:-999.44\n",
      "Epoch:5001-6000 Epsilon:0.9 Best Reward:-473.0 Mean Reward:-999.93 Best Mean Reward:-999.44\n",
      "Epoch:6001-7000 Epsilon:0.88 Best Reward:-473.0 Mean Reward:-999.92 Best Mean Reward:-999.44\n",
      "Epoch:7001-8000 Epsilon:0.86 Best Reward:-473.0 Mean Reward:-999.92 Best Mean Reward:-999.44\n",
      "Epoch:8001-9000 Epsilon:0.84 Best Reward:-473.0 Mean Reward:-999.8 Best Mean Reward:-999.44\n",
      "Epoch:9001-10000 Epsilon:0.82 Best Reward:-473.0 Mean Reward:-999.67 Best Mean Reward:-999.44\n",
      "Epoch:10001-11000 Epsilon:0.8 Best Reward:-473.0 Mean Reward:-999.53 Best Mean Reward:-999.44\n",
      "Epoch:11001-12000 Epsilon:0.78 Best Reward:-473.0 Mean Reward:-999.37 Best Mean Reward:-999.37\n",
      "Epoch:12001-13000 Epsilon:0.76 Best Reward:-473.0 Mean Reward:-999.19 Best Mean Reward:-999.19\n",
      "Epoch:13001-14000 Epsilon:0.74 Best Reward:-473.0 Mean Reward:-999.2 Best Mean Reward:-999.19\n",
      "Epoch:14001-15000 Epsilon:0.72 Best Reward:-453.0 Mean Reward:-999.2 Best Mean Reward:-999.19\n",
      "Epoch:15001-16000 Epsilon:0.7 Best Reward:-453.0 Mean Reward:-999.25 Best Mean Reward:-999.19\n",
      "Epoch:16001-17000 Epsilon:0.68 Best Reward:-453.0 Mean Reward:-999.42 Best Mean Reward:-999.19\n",
      "Epoch:17001-18000 Epsilon:0.66 Best Reward:-453.0 Mean Reward:-999.34 Best Mean Reward:-999.19\n",
      "Epoch:18001-19000 Epsilon:0.64 Best Reward:-453.0 Mean Reward:-999.38 Best Mean Reward:-999.19\n",
      "Epoch:19001-20000 Epsilon:0.62 Best Reward:-453.0 Mean Reward:-999.35 Best Mean Reward:-999.19\n",
      "Epoch:20001-21000 Epsilon:0.6 Best Reward:-453.0 Mean Reward:-999.2 Best Mean Reward:-999.19\n",
      "Epoch:21001-22000 Epsilon:0.58 Best Reward:-453.0 Mean Reward:-998.95 Best Mean Reward:-998.95\n",
      "Epoch:22001-23000 Epsilon:0.56 Best Reward:-453.0 Mean Reward:-998.87 Best Mean Reward:-998.87\n",
      "Epoch:23001-24000 Epsilon:0.54 Best Reward:-453.0 Mean Reward:-998.85 Best Mean Reward:-998.85\n",
      "Epoch:24001-25000 Epsilon:0.52 Best Reward:-453.0 Mean Reward:-998.68 Best Mean Reward:-998.68\n",
      "Epoch:25001-26000 Epsilon:0.5 Best Reward:-453.0 Mean Reward:-998.67 Best Mean Reward:-998.67\n",
      "Epoch:26001-27000 Epsilon:0.48 Best Reward:-453.0 Mean Reward:-998.59 Best Mean Reward:-998.59\n",
      "Epoch:27001-28000 Epsilon:0.46 Best Reward:-453.0 Mean Reward:-998.83 Best Mean Reward:-998.59\n",
      "Epoch:28001-29000 Epsilon:0.44 Best Reward:-453.0 Mean Reward:-998.64 Best Mean Reward:-998.59\n",
      "Epoch:29001-30000 Epsilon:0.42 Best Reward:-453.0 Mean Reward:-998.72 Best Mean Reward:-998.59\n",
      "Epoch:30001-31000 Epsilon:0.4 Best Reward:-453.0 Mean Reward:-998.9 Best Mean Reward:-998.59\n",
      "Epoch:31001-32000 Epsilon:0.38 Best Reward:-453.0 Mean Reward:-999.17 Best Mean Reward:-998.59\n",
      "Epoch:32001-33000 Epsilon:0.36 Best Reward:-453.0 Mean Reward:-999.2 Best Mean Reward:-998.59\n",
      "Epoch:33001-34000 Epsilon:0.34 Best Reward:-453.0 Mean Reward:-999.44 Best Mean Reward:-998.59\n",
      "Epoch:34001-35000 Epsilon:0.32 Best Reward:-453.0 Mean Reward:-999.71 Best Mean Reward:-998.59\n",
      "Epoch:35001-36000 Epsilon:0.3 Best Reward:-453.0 Mean Reward:-999.57 Best Mean Reward:-998.59\n",
      "Epoch:36001-37000 Epsilon:0.28 Best Reward:-453.0 Mean Reward:-999.6 Best Mean Reward:-998.59\n",
      "Epoch:37001-38000 Epsilon:0.26 Best Reward:-453.0 Mean Reward:-999.64 Best Mean Reward:-998.59\n",
      "Epoch:38001-39000 Epsilon:0.24 Best Reward:-453.0 Mean Reward:-999.63 Best Mean Reward:-998.59\n",
      "Epoch:39001-40000 Epsilon:0.22 Best Reward:-453.0 Mean Reward:-999.48 Best Mean Reward:-998.59\n",
      "Epoch:40001-41000 Epsilon:0.2 Best Reward:-453.0 Mean Reward:-999.73 Best Mean Reward:-998.59\n",
      "Epoch:41001-42000 Epsilon:0.18 Best Reward:-453.0 Mean Reward:-999.77 Best Mean Reward:-998.59\n",
      "Epoch:42001-43000 Epsilon:0.16 Best Reward:-453.0 Mean Reward:-999.8 Best Mean Reward:-998.59\n",
      "Epoch:43001-44000 Epsilon:0.14 Best Reward:-453.0 Mean Reward:-999.83 Best Mean Reward:-998.59\n",
      "Epoch:44001-45000 Epsilon:0.12 Best Reward:-453.0 Mean Reward:-999.99 Best Mean Reward:-998.59\n",
      "Epoch:45001-46000 Epsilon:0.1 Best Reward:-453.0 Mean Reward:-999.96 Best Mean Reward:-998.59\n",
      "Epoch:46001-47000 Epsilon:0.08 Best Reward:-453.0 Mean Reward:-999.97 Best Mean Reward:-998.59\n",
      "Epoch:47001-48000 Epsilon:0.06 Best Reward:-453.0 Mean Reward:-999.97 Best Mean Reward:-998.59\n",
      "Epoch:48001-49000 Epsilon:0.04 Best Reward:-453.0 Mean Reward:-999.97 Best Mean Reward:-998.59\n",
      "Generating Results for Learning Rate 0.01\n",
      "Epoch:1-1000 Epsilon:1 Best Reward:-1000.0 Mean Reward:-1000.0 Best Mean Reward:-1000.0\n",
      "Epoch:1001-2000 Epsilon:1.0 Best Reward:-971.0 Mean Reward:-999.99 Best Mean Reward:-999.99\n",
      "Epoch:2001-3000 Epsilon:0.98 Best Reward:-971.0 Mean Reward:-999.99 Best Mean Reward:-999.99\n",
      "Epoch:3001-4000 Epsilon:0.96 Best Reward:-771.0 Mean Reward:-999.92 Best Mean Reward:-999.92\n",
      "Epoch:4001-5000 Epsilon:0.94 Best Reward:-737.0 Mean Reward:-999.83 Best Mean Reward:-999.83\n",
      "Epoch:5001-6000 Epsilon:0.92 Best Reward:-686.0 Mean Reward:-999.56 Best Mean Reward:-999.56\n",
      "Epoch:6001-7000 Epsilon:0.9 Best Reward:-577.0 Mean Reward:-999.07 Best Mean Reward:-999.07\n",
      "Epoch:7001-8000 Epsilon:0.88 Best Reward:-577.0 Mean Reward:-999.01 Best Mean Reward:-999.01\n",
      "Epoch:8001-9000 Epsilon:0.86 Best Reward:-577.0 Mean Reward:-998.97 Best Mean Reward:-998.97\n",
      "Epoch:9001-10000 Epsilon:0.84 Best Reward:-577.0 Mean Reward:-998.98 Best Mean Reward:-998.97\n",
      "Epoch:10001-11000 Epsilon:0.82 Best Reward:-577.0 Mean Reward:-998.98 Best Mean Reward:-998.97\n",
      "Epoch:11001-12000 Epsilon:0.8 Best Reward:-577.0 Mean Reward:-999.24 Best Mean Reward:-998.97\n",
      "Epoch:12001-13000 Epsilon:0.78 Best Reward:-529.0 Mean Reward:-999.05 Best Mean Reward:-998.97\n",
      "Epoch:13001-14000 Epsilon:0.76 Best Reward:-404.0 Mean Reward:-997.8 Best Mean Reward:-997.8\n",
      "Epoch:14001-15000 Epsilon:0.74 Best Reward:-404.0 Mean Reward:-996.91 Best Mean Reward:-996.91\n",
      "Epoch:15001-16000 Epsilon:0.72 Best Reward:-404.0 Mean Reward:-996.91 Best Mean Reward:-996.91\n",
      "Epoch:16001-17000 Epsilon:0.7 Best Reward:-327.0 Mean Reward:-995.34 Best Mean Reward:-995.34\n",
      "Epoch:17001-18000 Epsilon:0.68 Best Reward:-327.0 Mean Reward:-994.62 Best Mean Reward:-994.62\n",
      "Epoch:18001-19000 Epsilon:0.66 Best Reward:-327.0 Mean Reward:-995.97 Best Mean Reward:-994.62\n",
      "Epoch:19001-20000 Epsilon:0.64 Best Reward:-267.0 Mean Reward:-990.36 Best Mean Reward:-990.36\n",
      "Epoch:20001-21000 Epsilon:0.62 Best Reward:-267.0 Mean Reward:-990.44 Best Mean Reward:-990.36\n",
      "Epoch:21001-22000 Epsilon:0.6 Best Reward:-267.0 Mean Reward:-992.12 Best Mean Reward:-990.36\n",
      "Epoch:22001-23000 Epsilon:0.58 Best Reward:-267.0 Mean Reward:-992.56 Best Mean Reward:-990.36\n",
      "Epoch:23001-24000 Epsilon:0.56 Best Reward:-267.0 Mean Reward:-992.5 Best Mean Reward:-990.36\n",
      "Epoch:24001-25000 Epsilon:0.54 Best Reward:-267.0 Mean Reward:-999.1 Best Mean Reward:-990.36\n",
      "Epoch:25001-26000 Epsilon:0.52 Best Reward:-267.0 Mean Reward:-999.28 Best Mean Reward:-990.36\n",
      "Epoch:26001-27000 Epsilon:0.5 Best Reward:-267.0 Mean Reward:-999.42 Best Mean Reward:-990.36\n",
      "Epoch:27001-28000 Epsilon:0.48 Best Reward:-267.0 Mean Reward:-999.94 Best Mean Reward:-990.36\n",
      "Epoch:28001-29000 Epsilon:0.46 Best Reward:-267.0 Mean Reward:-1000.0 Best Mean Reward:-990.36\n",
      "Epoch:29001-30000 Epsilon:0.44 Best Reward:-267.0 Mean Reward:-1000.0 Best Mean Reward:-990.36\n",
      "Epoch:30001-31000 Epsilon:0.42 Best Reward:-267.0 Mean Reward:-999.89 Best Mean Reward:-990.36\n",
      "Epoch:31001-32000 Epsilon:0.4 Best Reward:-267.0 Mean Reward:-999.89 Best Mean Reward:-990.36\n",
      "Epoch:32001-33000 Epsilon:0.38 Best Reward:-267.0 Mean Reward:-999.89 Best Mean Reward:-990.36\n",
      "Epoch:33001-34000 Epsilon:0.36 Best Reward:-267.0 Mean Reward:-999.89 Best Mean Reward:-990.36\n",
      "Best rewards so far: -153.0\n",
      "Best model saved as mountaincar_dql_34052.pt\n",
      "Epoch:34001-35000 Epsilon:0.34 Best Reward:-153.0 Mean Reward:-998.24 Best Mean Reward:-990.36\n",
      "Epoch:35001-36000 Epsilon:0.32 Best Reward:-153.0 Mean Reward:-998.34 Best Mean Reward:-990.36\n",
      "Epoch:36001-37000 Epsilon:0.3 Best Reward:-153.0 Mean Reward:-997.98 Best Mean Reward:-990.36\n",
      "Epoch:37001-38000 Epsilon:0.28 Best Reward:-153.0 Mean Reward:-997.98 Best Mean Reward:-990.36\n",
      "Epoch:38001-39000 Epsilon:0.26 Best Reward:-153.0 Mean Reward:-997.48 Best Mean Reward:-990.36\n",
      "Epoch:39001-40000 Epsilon:0.24 Best Reward:-153.0 Mean Reward:-999.12 Best Mean Reward:-990.36\n",
      "Epoch:40001-41000 Epsilon:0.22 Best Reward:-153.0 Mean Reward:-999.03 Best Mean Reward:-990.36\n",
      "Epoch:41001-42000 Epsilon:0.2 Best Reward:-153.0 Mean Reward:-999.3 Best Mean Reward:-990.36\n",
      "Epoch:42001-43000 Epsilon:0.18 Best Reward:-153.0 Mean Reward:-998.66 Best Mean Reward:-990.36\n",
      "Epoch:43001-44000 Epsilon:0.16 Best Reward:-153.0 Mean Reward:-999.17 Best Mean Reward:-990.36\n",
      "Epoch:44001-45000 Epsilon:0.14 Best Reward:-153.0 Mean Reward:-999.17 Best Mean Reward:-990.36\n",
      "Epoch:45001-46000 Epsilon:0.12 Best Reward:-153.0 Mean Reward:-999.28 Best Mean Reward:-990.36\n",
      "Epoch:46001-47000 Epsilon:0.1 Best Reward:-153.0 Mean Reward:-999.36 Best Mean Reward:-990.36\n",
      "Epoch:47001-48000 Epsilon:0.08 Best Reward:-153.0 Mean Reward:-1000.0 Best Mean Reward:-990.36\n",
      "Epoch:48001-49000 Epsilon:0.06 Best Reward:-153.0 Mean Reward:-999.85 Best Mean Reward:-990.36\n",
      "Generating Results for Learning Rate 0.001\n",
      "Epoch:1-1000 Epsilon:1 Best Reward:-1000.0 Mean Reward:-1000.0 Best Mean Reward:-1000.0\n",
      "Epoch:1001-2000 Epsilon:0.99 Best Reward:-834.0 Mean Reward:-999.92 Best Mean Reward:-999.92\n",
      "Epoch:2001-3000 Epsilon:0.97 Best Reward:-834.0 Mean Reward:-999.94 Best Mean Reward:-999.92\n",
      "Epoch:3001-4000 Epsilon:0.95 Best Reward:-834.0 Mean Reward:-999.96 Best Mean Reward:-999.92\n",
      "Epoch:4001-5000 Epsilon:0.93 Best Reward:-722.0 Mean Reward:-999.91 Best Mean Reward:-999.91\n",
      "Epoch:5001-6000 Epsilon:0.91 Best Reward:-722.0 Mean Reward:-999.91 Best Mean Reward:-999.91\n",
      "Epoch:6001-7000 Epsilon:0.89 Best Reward:-722.0 Mean Reward:-999.94 Best Mean Reward:-999.91\n",
      "Epoch:7001-8000 Epsilon:0.87 Best Reward:-722.0 Mean Reward:-999.94 Best Mean Reward:-999.91\n",
      "Epoch:8001-9000 Epsilon:0.85 Best Reward:-722.0 Mean Reward:-999.94 Best Mean Reward:-999.91\n",
      "Epoch:9001-10000 Epsilon:0.83 Best Reward:-722.0 Mean Reward:-1000.0 Best Mean Reward:-999.91\n",
      "Epoch:10001-11000 Epsilon:0.81 Best Reward:-722.0 Mean Reward:-1000.0 Best Mean Reward:-999.91\n",
      "Epoch:11001-12000 Epsilon:0.79 Best Reward:-722.0 Mean Reward:-1000.0 Best Mean Reward:-999.91\n",
      "Epoch:12001-13000 Epsilon:0.77 Best Reward:-722.0 Mean Reward:-999.9 Best Mean Reward:-999.9\n",
      "Epoch:13001-14000 Epsilon:0.75 Best Reward:-722.0 Mean Reward:-999.85 Best Mean Reward:-999.85\n",
      "Epoch:14001-15000 Epsilon:0.73 Best Reward:-722.0 Mean Reward:-999.85 Best Mean Reward:-999.85\n",
      "Epoch:15001-16000 Epsilon:0.71 Best Reward:-334.0 Mean Reward:-994.44 Best Mean Reward:-994.44\n",
      "Epoch:16001-17000 Epsilon:0.69 Best Reward:-334.0 Mean Reward:-989.33 Best Mean Reward:-989.33\n",
      "Epoch:17001-18000 Epsilon:0.67 Best Reward:-259.0 Mean Reward:-976.24 Best Mean Reward:-976.24\n",
      "Epoch:18001-19000 Epsilon:0.65 Best Reward:-259.0 Mean Reward:-975.55 Best Mean Reward:-975.55\n",
      "Epoch:19001-20000 Epsilon:0.63 Best Reward:-259.0 Mean Reward:-975.2 Best Mean Reward:-975.2\n",
      "Epoch:20001-21000 Epsilon:0.61 Best Reward:-238.0 Mean Reward:-978.35 Best Mean Reward:-975.2\n",
      "Best rewards so far: -186.0\n",
      "Best model saved as mountaincar_dql_21147.pt\n",
      "Epoch:21001-22000 Epsilon:0.59 Best Reward:-186.0 Mean Reward:-979.78 Best Mean Reward:-975.2\n",
      "Best rewards so far: -178.0\n",
      "Best model saved as mountaincar_dql_22768.pt\n",
      "Epoch:22001-23000 Epsilon:0.57 Best Reward:-178.0 Mean Reward:-979.39 Best Mean Reward:-975.2\n",
      "Best rewards so far: -165.0\n",
      "Best model saved as mountaincar_dql_23520.pt\n",
      "Epoch:23001-24000 Epsilon:0.55 Best Reward:-165.0 Mean Reward:-964.37 Best Mean Reward:-964.37\n",
      "Epoch:24001-25000 Epsilon:0.53 Best Reward:-165.0 Mean Reward:-942.1 Best Mean Reward:-942.1\n",
      "Epoch:25001-26000 Epsilon:0.51 Best Reward:-165.0 Mean Reward:-916.95 Best Mean Reward:-916.95\n",
      "Epoch:26001-27000 Epsilon:0.49 Best Reward:-165.0 Mean Reward:-892.78 Best Mean Reward:-892.78\n",
      "Epoch:27001-28000 Epsilon:0.47 Best Reward:-165.0 Mean Reward:-881.06 Best Mean Reward:-881.06\n",
      "Epoch:28001-29000 Epsilon:0.45 Best Reward:-165.0 Mean Reward:-871.28 Best Mean Reward:-871.28\n",
      "Epoch:29001-30000 Epsilon:0.43 Best Reward:-165.0 Mean Reward:-867.83 Best Mean Reward:-867.83\n",
      "Epoch:30001-31000 Epsilon:0.41 Best Reward:-165.0 Mean Reward:-871.9 Best Mean Reward:-867.83\n",
      "Epoch:31001-32000 Epsilon:0.39 Best Reward:-165.0 Mean Reward:-869.92 Best Mean Reward:-867.83\n",
      "Best rewards so far: -163.0\n",
      "Best model saved as mountaincar_dql_32309.pt\n",
      "Epoch:32001-33000 Epsilon:0.37 Best Reward:-163.0 Mean Reward:-861.53 Best Mean Reward:-861.53\n",
      "Epoch:33001-34000 Epsilon:0.35 Best Reward:-163.0 Mean Reward:-825.79 Best Mean Reward:-825.79\n",
      "Epoch:34001-35000 Epsilon:0.33 Best Reward:-163.0 Mean Reward:-835.33 Best Mean Reward:-825.79\n",
      "Epoch:35001-36000 Epsilon:0.31 Best Reward:-163.0 Mean Reward:-828.96 Best Mean Reward:-825.79\n",
      "Epoch:36001-37000 Epsilon:0.29 Best Reward:-163.0 Mean Reward:-806.86 Best Mean Reward:-806.86\n",
      "Epoch:37001-38000 Epsilon:0.27 Best Reward:-163.0 Mean Reward:-810.27 Best Mean Reward:-806.86\n",
      "Epoch:38001-39000 Epsilon:0.25 Best Reward:-163.0 Mean Reward:-833.26 Best Mean Reward:-806.86\n",
      "Best rewards so far: -157.0\n",
      "Best model saved as mountaincar_dql_39128.pt\n",
      "Best rewards so far: -144.0\n",
      "Best model saved as mountaincar_dql_39138.pt\n",
      "Best rewards so far: -143.0\n",
      "Best model saved as mountaincar_dql_39155.pt\n",
      "Epoch:39001-40000 Epsilon:0.23 Best Reward:-143.0 Mean Reward:-787.98 Best Mean Reward:-787.98\n",
      "Epoch:40001-41000 Epsilon:0.21 Best Reward:-143.0 Mean Reward:-774.38 Best Mean Reward:-774.38\n",
      "Epoch:41001-42000 Epsilon:0.19 Best Reward:-143.0 Mean Reward:-779.28 Best Mean Reward:-774.38\n",
      "Epoch:42001-43000 Epsilon:0.17 Best Reward:-143.0 Mean Reward:-754.74 Best Mean Reward:-754.74\n",
      "Epoch:43001-44000 Epsilon:0.15 Best Reward:-143.0 Mean Reward:-747.5 Best Mean Reward:-747.5\n",
      "Best rewards so far: -141.0\n",
      "Best model saved as mountaincar_dql_44202.pt\n",
      "Epoch:44001-45000 Epsilon:0.13 Best Reward:-141.0 Mean Reward:-717.76 Best Mean Reward:-717.76\n",
      "Epoch:45001-46000 Epsilon:0.11 Best Reward:-141.0 Mean Reward:-702.45 Best Mean Reward:-702.45\n",
      "Epoch:46001-47000 Epsilon:0.09 Best Reward:-141.0 Mean Reward:-712.53 Best Mean Reward:-702.45\n",
      "Epoch:47001-48000 Epsilon:0.07 Best Reward:-141.0 Mean Reward:-675.1 Best Mean Reward:-675.1\n",
      "Epoch:48001-49000 Epsilon:0.05 Best Reward:-141.0 Mean Reward:-600.0 Best Mean Reward:-600.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    learning_rates_list = [0.1,0.01,0.001]\n",
    "    for learning_rate in learning_rates_list:\n",
    "        print(f\"Generating Results for Learning Rate {learning_rate}\")\n",
    "        mountaincar = MountainCarDQL(learning_rate=learning_rate)\n",
    "        mountaincar.train(50000, False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb59fe3-c1d8-4498-9056-9018aa143012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
